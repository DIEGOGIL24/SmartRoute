# --- Config del modelo (MODELOS CON M√ÅS TOKENS) ---
# IMPORTANTE: Usa AutoModelForCausalLM (NO Seq2SeqLM)
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_huggingface import HuggingFacePipeline
from langchain_core.prompts import PromptTemplate  # ‚úÖ CORRECTO
from langchain_core.output_parsers import StrOutputParser
import json
import sys
from pathlib import Path

# model_name = "meta-llama/Llama-3.2-3B-Instruct"
# max_length = 127000

# model_name = "Qwen/Qwen2.5-1.5B-Instruct"
# max_length = 32768

# model_name = "Qwen/Qwen2.5-3B-Instruct"
# max_length = 32768

model_name = "Qwen/Qwen3-4B-Instruct-2507"
max_length = 32768

# Cargar tokenizer expl√≠citamente
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

# Pipeline con configuraci√≥n mejorada
pipe = pipeline(
    "text-generation",
    model=model_name,
    tokenizer=tokenizer,
    device_map="auto",
    torch_dtype="auto",
    trust_remote_code=True,  # Importante para modelos Qwen
    model_kwargs={
        "low_cpu_mem_usage": True,
        "use_cache": True  # Acelera generaci√≥n
    }
)

# Par√°metros de generaci√≥n optimizados
generation_params = {
    "max_new_tokens": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "top_k": 50,
    "repetition_penalty": 1.1,
    "do_sample": True,
    "num_return_sequences": 1,
    "pad_token_id": tokenizer.pad_token_id or tokenizer.eos_token_id,
    "eos_token_id": tokenizer.eos_token_id,
    "return_full_text": False  # CR√çTICO: Solo devuelve texto generado
}

# HuggingFacePipeline con par√°metros
llm = HuggingFacePipeline(
    pipeline=pipe,
    model_kwargs=generation_params
)

# Prompt mejorado (m√°s conciso y directo)
prompt = PromptTemplate.from_template(
    """Eres un asistente de viajes. Analiza el siguiente JSON con datos de clima y lugares tur√≠sticos, y genera un itinerario tur√≠stico detallado en espa√±ol.

<JSON>
{json_blob}
</JSON>

Genera el itinerario usando este formato:

üåç Itinerario para [CIUDAD]
üìÖ Per√≠odo: [Primera fecha - √öltima fecha]

D√≠a 1 ([fecha]):
- Ma√±ana: [Actividad espec√≠fica en lugar tur√≠stico] - Clima: [temperatura y condici√≥n]
- Tarde: [Actividad espec√≠fica en lugar tur√≠stico] - Clima: [temperatura y condici√≥n]
- Noche: [Actividad espec√≠fica en lugar tur√≠stico] - Clima: [temperatura y condici√≥n]

[Repite para cada d√≠a]

üå°Ô∏è Resumen del clima: [Descripci√≥n general de las condiciones]

üí° Recomendaciones:
- [Consejo pr√°ctico 1]
- [Consejo pr√°ctico 2]
- [Consejo pr√°ctico 3]

Responde √öNICAMENTE con el itinerario, sin pre√°mbulos."""
)

# Chain
chain = prompt | llm | StrOutputParser()


def check_token_length(text):
    """
    Verifica la longitud del texto en tokens.
    """
    tokens = tokenizer.encode(text, truncation=False)
    token_count = len(tokens)
    print(f"üìä Longitud del JSON: {token_count} tokens (m√°ximo: {max_length})")

    if token_count > max_length - 600:  # Dejar espacio para la respuesta
        print(f"‚ö†Ô∏è  ADVERTENCIA: El JSON es muy largo ({token_count} tokens)")
        print(f"   Se recomienda resumirlo o usar un modelo con m√°s capacidad")
        return False

    return True


def process_json_file(json_path):
    """
    Lee un archivo JSON y lo convierte en texto conversacional.

    Args:
        json_path: Ruta al archivo JSON

    Returns:
        Texto generado por el modelo
    """
    # Leer archivo JSON
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    json_blob = json.dumps(data, indent=2, ensure_ascii=False)

    print(f"\nüìÑ Procesando archivo: {json_path}")
    print(f"\nüìù JSON de entrada (primeros 500 caracteres):")
    print(json_blob[:500] + "...\n")

    # Verificar longitud
    if not check_token_length(json_blob):
        respuesta = input("\n‚ùì ¬øContinuar de todas formas? (s/n): ")
        if respuesta.lower() != 's':
            print("‚ùå Proceso cancelado")
            return None

    print("ü§ñ Generando texto conversacional...\n")

    try:
        result = chain.invoke({"json_blob": json_blob})

        # Limpiar la respuesta - extraer solo la parte del asistente
        if "<|im_start|>assistant" in result:
            result = result.split("<|im_start|>assistant")[-1].strip()
        if "<|im_end|>" in result:
            result = result.split("<|im_end|>")[0].strip()
        if "Texto conversacional:" in result:
            result = result.split("Texto conversacional:")[-1].strip()

        return result

    except Exception as e:
        print(f"‚ùå Error al generar texto: {e}")
        return None


def main(json_path: str):
    """
    Funci√≥n principal para probar el generador con un archivo JSON.
    Uso: python script.py <archivo.json>
    """

    try:
        # Procesar el archivo
        resultado = process_json_file(json_path)

        if resultado is None:
            sys.exit(1)

        # Mostrar resultado
        print("‚úÖ Resultado:\n")
        print("=" * 60)
        print(resultado)
        print("=" * 60)

        # Guardar resultado en archivo de texto
        output_path = json_path.replace('.json', '_resultado.txt')
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(resultado)

        print(f"\nüíæ Resultado guardado en: {output_path}")

    except json.JSONDecodeError as e:
        print(f"‚ùå Error al leer JSON: {e}")
        sys.exit(1)
    except Exception as e:
        print(f"‚ùå Error inesperado: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main("out.json")